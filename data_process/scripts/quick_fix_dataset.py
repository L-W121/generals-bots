"""
å¿«é€Ÿä¿®å¤è„šæœ¬ - ç«‹å³è§£å†³ PyTorch DataLoader å°ºå¯¸é—®é¢˜
"""

import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

class QuickFixDataset(Dataset):
    """å¿«é€Ÿä¿®å¤ç‰ˆæœ¬ - åªä½¿ç”¨æœ€å¸¸è§çš„åœ°å›¾å°ºå¯¸"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.samples = []
        
        # ç›®æ ‡å°ºå¯¸ï¼šæœ€å¸¸è§çš„ 21Ã—18 (height Ã— width)
        target_h, target_w = 21, 18
        
        print("ğŸ”§ å¿«é€Ÿä¿®å¤ï¼šè¿‡æ»¤ç›¸åŒå°ºå¯¸æ ·æœ¬...")
        
        # æ‰¾åˆ°æ‰¹æ¬¡æ–‡ä»¶
        batch_files = list(self.data_dir.glob("batch_*.pt"))
        if not batch_files:
            batch_files = list(self.data_dir.glob("optimized_batch_*.pt"))
        
        total_samples = 0
        valid_samples = 0
        
        for batch_file in batch_files:
            try:
                batch_data = torch.load(batch_file, map_location='cpu')
                
                for state_tensor, action_id, metadata in batch_data:
                    total_samples += 1
                    _, h, w = state_tensor.shape
                    
                    # åªä¿ç•™ 21Ã—18 å°ºå¯¸çš„æ ·æœ¬
                    if h == target_h and w == target_w:
                        self.samples.append((state_tensor, action_id))
                        valid_samples += 1
                
            except Exception as e:
                print(f"âš ï¸  è·³è¿‡æ–‡ä»¶: {batch_file.name}")
                continue
        
        print(f"âœ… ä¿®å¤å®Œæˆ!")
        print(f"   åŸå§‹æ ·æœ¬: {total_samples:,}")
        print(f"   æœ‰æ•ˆæ ·æœ¬: {valid_samples:,} (21Ã—18 å°ºå¯¸)")
        print(f"   ä¿ç•™æ¯”ä¾‹: {valid_samples/total_samples*100:.1f}%")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        return self.samples[idx]

def quick_test(data_dir: str):
    """å¿«é€Ÿæµ‹è¯•ä¿®å¤æ•ˆæœ"""
    print("ğŸ§ª å¿«é€Ÿæµ‹è¯•ä¿®å¤æ•ˆæœ...")
    
    try:
        # åˆ›å»ºä¿®å¤åçš„æ•°æ®é›†
        dataset = QuickFixDataset(data_dir)
        
        if len(dataset) == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°21Ã—18å°ºå¯¸çš„æ ·æœ¬ï¼Œå°è¯•å…¶ä»–å°ºå¯¸...")
            return False
        
        # æµ‹è¯• DataLoader
        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
        
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡
        batch_states, batch_actions = next(iter(dataloader))
        
        print(f"âœ… æµ‹è¯•æˆåŠŸ!")
        print(f"   æ‰¹æ¬¡çŠ¶æ€å½¢çŠ¶: {batch_states.shape}")
        print(f"   æ‰¹æ¬¡åŠ¨ä½œå½¢çŠ¶: {batch_actions.shape}")
        print(f"   æ•°æ®ç±»å‹: {batch_states.dtype}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def analyze_map_sizes(data_dir: str):
    """åˆ†æåœ°å›¾å°ºå¯¸åˆ†å¸ƒ"""
    print("ğŸ“Š åˆ†æåœ°å›¾å°ºå¯¸åˆ†å¸ƒ...")
    
    data_path = Path(data_dir)
    batch_files = list(data_path.glob("batch_*.pt")) + list(data_path.glob("optimized_batch_*.pt"))
    
    size_count = {}
    total_samples = 0
    
    for batch_file in batch_files[:3]:  # åªåˆ†æå‰3ä¸ªæ–‡ä»¶
        try:
            batch_data = torch.load(batch_file, map_location='cpu')
            
            for state_tensor, action_id, metadata in batch_data:
                total_samples += 1
                _, h, w = state_tensor.shape
                size = (h, w)
                size_count[size] = size_count.get(size, 0) + 1
                
        except Exception:
            continue
    
    print(f"\\nğŸ“ åœ°å›¾å°ºå¯¸åˆ†å¸ƒ (åŸºäº {total_samples} ä¸ªæ ·æœ¬):")
    for size, count in sorted(size_count.items(), key=lambda x: -x[1]):
        h, w = size
        percentage = count / total_samples * 100
        print(f"   {h}Ã—{w}: {count:,} æ ·æœ¬ ({percentage:.1f}%)")
    
    # æ¨èæœ€ä½³å°ºå¯¸
    if size_count:
        best_size = max(size_count.items(), key=lambda x: x[1])
        print(f"\\nğŸ¯ æ¨èä½¿ç”¨: {best_size[0][0]}Ã—{best_size[0][1]} (æœ€å¤šæ ·æœ¬)")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='å¿«é€Ÿä¿®å¤ DataLoader å°ºå¯¸é—®é¢˜')
    parser.add_argument('--data_dir', type=str, required=True,
                       help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--analyze', action='store_true',
                       help='åˆ†æåœ°å›¾å°ºå¯¸åˆ†å¸ƒ')
    
    args = parser.parse_args()
    
    if args.analyze:
        analyze_map_sizes(args.data_dir)
    else:
        success = quick_test(args.data_dir)
        if not success:
            print("\\nğŸ” å»ºè®®è¿è¡Œåˆ†æ:")
            print(f"python {__file__} --data_dir {args.data_dir} --analyze")