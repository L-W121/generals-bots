"""
æ™ºèƒ½å¡«å……ç­–ç•¥ - ç”¨å±±è„‰å¡«å……è¾¹ç•Œä½¿åœ°å›¾å°ºå¯¸ç»Ÿä¸€
è¿™æ˜¯æœ€ä¼˜é›…çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¢ä¿ç•™æ‰€æœ‰æ•°æ®åˆç¬¦åˆæ¸¸æˆé€»è¾‘

"""

import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import numpy as np
import sys

class MountainPaddedDataset(Dataset):
    """ç”¨å±±è„‰å¡«å……çš„æ™ºèƒ½æ•°æ®é›† - æœ€ä½³è§£å†³æ–¹æ¡ˆ"""
    
    def __init__(self, data_dir: str, 
                 unified_size: tuple = (22, 22),  # ç»Ÿä¸€å°ºå¯¸
                 mountain_value: float = -1.0):   # å±±è„‰åœ¨çŠ¶æ€ä¸­çš„å€¼
        """
        Args:
            unified_size: ç»Ÿä¸€åçš„åœ°å›¾å°ºå¯¸ (height, width)
            mountain_value: å±±è„‰é€šé“ä¸­çš„æ•°å€¼
        """
        self.data_dir = Path(data_dir)
        self.unified_h, self.unified_w = unified_size
        self.mountain_value = mountain_value
        self.samples = []
        
        print(f"ğŸ”ï¸  æ™ºèƒ½å±±è„‰å¡«å……ç­–ç•¥")
        print(f"   ç›®æ ‡å°ºå¯¸: {unified_size}")
        print(f"   ç­–ç•¥: ç”¨å±±è„‰å¡«å……è¾¹ç•ŒåŒºåŸŸ")
        
        self._load_and_pad_all_samples()
    
    def _load_and_pad_all_samples(self):
        """åŠ è½½æ‰€æœ‰æ ·æœ¬å¹¶ç”¨å±±è„‰æ™ºèƒ½å¡«å……"""
        batch_files = list(self.data_dir.glob("batch_*.pt")) + \
                     list(self.data_dir.glob("optimized_batch_*.pt"))
        
        total_samples = 0
        size_stats = {}
        
        for batch_file in batch_files:
            try:
                batch_data = torch.load(batch_file, map_location='cpu')
                
                for state_tensor, action_id, metadata in batch_data:
                    total_samples += 1
                    
                    # è®°å½•åŸå§‹å°ºå¯¸
                    _, orig_h, orig_w = state_tensor.shape
                    size_key = (orig_h, orig_w)
                    size_stats[size_key] = size_stats.get(size_key, 0) + 1
                    
                    # æ™ºèƒ½å¡«å……
                    padded_tensor = self._smart_mountain_padding(state_tensor)
                    self.samples.append((padded_tensor, action_id, metadata))
                
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶: {batch_file.name}")
                continue
        
        print(f"\\nğŸ“Š å¤„ç†å®Œæˆ:")
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples:,}")
        print(f"   æˆåŠŸå¡«å……: {len(self.samples):,}")
        print(f"   ç»Ÿä¸€å°ºå¯¸: {self.unified_h}Ã—{self.unified_w}")
        
        print(f"\\nğŸ“ åŸå§‹å°ºå¯¸åˆ†å¸ƒ:")
        for size, count in sorted(size_stats.items(), key=lambda x: -x[1])[:10]:
            h, w = size
            percentage = count / total_samples * 100
            print(f"   {h:2d}Ã—{w:2d}: {count:4d} ({percentage:4.1f}%)")
    
    def _smart_mountain_padding(self, state_tensor):
        """æ™ºèƒ½å±±è„‰å¡«å……ç­–ç•¥"""
        channels, orig_h, orig_w = state_tensor.shape
        
        # å¦‚æœå·²ç»æ˜¯ç›®æ ‡å°ºå¯¸ï¼Œç›´æ¥è¿”å›
        if orig_h == self.unified_h and orig_w == self.unified_w:
            return state_tensor
        
        # å¦‚æœåŸå°ºå¯¸è¶…è¿‡ç›®æ ‡å°ºå¯¸ï¼Œéœ€è¦è£å‰ª
        if orig_h > self.unified_h or orig_w > self.unified_w:
            # ä»ä¸­å¿ƒè£å‰ª
            crop_h = min(orig_h, self.unified_h)
            crop_w = min(orig_w, self.unified_w)
            
            start_h = (orig_h - crop_h) // 2
            start_w = (orig_w - crop_w) // 2
            
            state_tensor = state_tensor[:, 
                                      start_h:start_h+crop_h,
                                      start_w:start_w+crop_w]
            orig_h, orig_w = crop_h, crop_w
        
        # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„å¼ é‡
        padded_tensor = torch.zeros(channels, self.unified_h, self.unified_w, 
                                  dtype=state_tensor.dtype)
        
        # è®¡ç®—å±…ä¸­æ”¾ç½®çš„ä½ç½®
        start_h = (self.unified_h - orig_h) // 2
        start_w = (self.unified_w - orig_w) // 2
        
        # æ”¾ç½®åŸå§‹æ•°æ®åˆ°ä¸­å¿ƒ
        padded_tensor[:, start_h:start_h+orig_h, start_w:start_w+orig_w] = state_tensor
        
        # å…³é”®ï¼šåœ¨å±±è„‰é€šé“(ç¬¬4é€šé“)ä¸­å¡«å……è¾¹ç•Œä¸ºå±±è„‰
        mountain_channel = 4  # æ ¹æ®state_encoder.pyï¼Œå±±è„‰æ˜¯ç¬¬4é€šé“
        
        # å¡«å……ä¸Šä¸‹è¾¹ç•Œ
        if start_h > 0:
            padded_tensor[mountain_channel, :start_h, :] = 1.0          # ä¸Šè¾¹ç•Œ
        if start_h + orig_h < self.unified_h:
            padded_tensor[mountain_channel, start_h+orig_h:, :] = 1.0   # ä¸‹è¾¹ç•Œ
        
        # å¡«å……å·¦å³è¾¹ç•Œ
        if start_w > 0:
            padded_tensor[mountain_channel, :, :start_w] = 1.0          # å·¦è¾¹ç•Œ
        if start_w + orig_w < self.unified_w:
            padded_tensor[mountain_channel, :, start_w+orig_w:] = 1.0   # å³è¾¹ç•Œ
        
        return padded_tensor
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        state_tensor, action_id, metadata = self.samples[idx]
        return state_tensor, action_id
    
    def get_sample_info(self, idx):
        """è·å–æ ·æœ¬è¯¦ç»†ä¿¡æ¯"""
        state_tensor, action_id, metadata = self.samples[idx]
        return {
            'state_shape': state_tensor.shape,
            'action_id': action_id,
            'metadata': metadata,
            'mountain_mask': state_tensor[4].sum().item()  # å±±è„‰æ•°é‡
        }

class MultiSizeDataset:
    """å¤šå°ºå¯¸åˆ†ç±»è®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, data_dir: str, min_samples_per_size: int = 1000):
        """
        æŒ‰åœ°å›¾å°ºå¯¸åˆ†ç±»åˆ›å»ºå¤šä¸ªæ•°æ®é›†
        
        Args:
            min_samples_per_size: æ¯ç§å°ºå¯¸æœ€å°‘æ ·æœ¬æ•°ï¼Œå°‘äºæ­¤æ•°çš„å°ºå¯¸ä¼šè¢«å¿½ç•¥
        """
        self.data_dir = Path(data_dir)
        self.min_samples_per_size = min_samples_per_size
        self.datasets_by_size = {}
        
        print(f"ğŸ“‚ åˆ›å»ºå¤šå°ºå¯¸åˆ†ç±»æ•°æ®é›†")
        print(f"   æœ€å°æ ·æœ¬æ•°é˜ˆå€¼: {min_samples_per_size}")
        
        self._organize_by_size()
    
    def _organize_by_size(self):
        """æŒ‰å°ºå¯¸ç»„ç»‡æ•°æ®"""
        # å…ˆç»Ÿè®¡å„å°ºå¯¸çš„æ ·æœ¬æ•°
        size_samples = {}
        
        batch_files = list(self.data_dir.glob("batch_*.pt")) + \
                     list(self.data_dir.glob("optimized_batch_*.pt"))
        
        print("ğŸ” ç»Ÿè®¡å„å°ºå¯¸æ ·æœ¬æ•°...")
        for batch_file in batch_files:
            try:
                batch_data = torch.load(batch_file, map_location='cpu')
                
                for state_tensor, action_id, metadata in batch_data:
                    _, h, w = state_tensor.shape
                    size_key = (h, w)
                    
                    if size_key not in size_samples:
                        size_samples[size_key] = []
                    
                    size_samples[size_key].append((state_tensor, action_id, metadata))
                
            except Exception:
                continue
        
        # ç­›é€‰æ ·æœ¬æ•°è¶³å¤Ÿçš„å°ºå¯¸
        valid_sizes = {size: samples for size, samples in size_samples.items() 
                      if len(samples) >= self.min_samples_per_size}
        
        print(f"\\nğŸ“Š å°ºå¯¸åˆ†å¸ƒ:")
        for size, samples in sorted(size_samples.items(), key=lambda x: -len(x[1])):
            h, w = size
            count = len(samples)
            status = "âœ…" if count >= self.min_samples_per_size else "âŒ"
            print(f"   {status} {h:2d}Ã—{w:2d}: {count:6,} æ ·æœ¬")
        
        # åˆ›å»ºå„å°ºå¯¸çš„æ•°æ®é›†
        print(f"\\nğŸ—ï¸  åˆ›å»º {len(valid_sizes)} ä¸ªå°ºå¯¸æ•°æ®é›†:")
        for size, samples in valid_sizes.items():
            h, w = size
            dataset = SingleSizeDataset(samples, size)
            self.datasets_by_size[size] = dataset
            print(f"   {h:2d}Ã—{w:2d}: {len(dataset):6,} æ ·æœ¬")
    
    def get_dataset(self, size: tuple):
        """è·å–æŒ‡å®šå°ºå¯¸çš„æ•°æ®é›†"""
        return self.datasets_by_size.get(size)
    
    def get_all_sizes(self):
        """è·å–æ‰€æœ‰å¯ç”¨å°ºå¯¸"""
        return list(self.datasets_by_size.keys())
    
    def get_largest_dataset(self):
        """è·å–æ ·æœ¬æ•°æœ€å¤šçš„æ•°æ®é›†"""
        if not self.datasets_by_size:
            return None
        
        largest_size = max(self.datasets_by_size.keys(), 
                          key=lambda size: len(self.datasets_by_size[size]))
        return self.datasets_by_size[largest_size], largest_size

class SingleSizeDataset(Dataset):
    """å•ä¸€å°ºå¯¸æ•°æ®é›†"""
    
    def __init__(self, samples, size):
        self.samples = samples
        self.size = size
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        state_tensor, action_id, metadata = self.samples[idx]
        return state_tensor, action_id

def test_mountain_padding(data_dir: str):
    """æµ‹è¯•å±±è„‰å¡«å……æ–¹æ¡ˆ"""
    print("ğŸ”ï¸  æµ‹è¯•æ™ºèƒ½å±±è„‰å¡«å……æ–¹æ¡ˆ")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå±±è„‰å¡«å……æ•°æ®é›†
        dataset = MountainPaddedDataset(data_dir, unified_size=(25, 25))
        
        if len(dataset) == 0:
            print("âŒ æ²¡æœ‰å¯ç”¨æ ·æœ¬")
            return False
        
        # æµ‹è¯•DataLoader
        dataloader = DataLoader(dataset, batch_size=16, shuffle=True)
        batch_states, batch_actions = next(iter(dataloader))
        
        print(f"\\nâœ… å±±è„‰å¡«å……æµ‹è¯•æˆåŠŸ!")
        print(f"   æ ·æœ¬æ•°é‡: {len(dataset):,}")
        print(f"   æ‰¹æ¬¡å½¢çŠ¶: {batch_states.shape}")
        print(f"   æ•°æ®ç±»å‹: {batch_states.dtype}")
        
        # æ£€æŸ¥å¡«å……æ•ˆæœ
        sample_info = dataset.get_sample_info(0)
        print(f"   æ ·æœ¬ä¿¡æ¯: {sample_info}")
        
        # å¯è§†åŒ–ä¸€ä¸ªæ ·æœ¬çš„å±±è„‰åˆ†å¸ƒ
        sample_state = batch_states[0]
        mountain_channel = sample_state[4]  # å±±è„‰é€šé“
        mountain_count = (mountain_channel > 0).sum().item()
        total_cells = mountain_channel.numel()
        
        print(f"   å±±è„‰è¦†ç›–: {mountain_count}/{total_cells} ({mountain_count/total_cells*100:.1f}%)")
        
        return True
        
    except Exception as e:
        print(f"âŒ å±±è„‰å¡«å……æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_multi_size(data_dir: str):
    """æµ‹è¯•å¤šå°ºå¯¸åˆ†ç±»æ–¹æ¡ˆ"""
    print("\\nğŸ“‚ æµ‹è¯•å¤šå°ºå¯¸åˆ†ç±»æ–¹æ¡ˆ")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå¤šå°ºå¯¸æ•°æ®é›†
        multi_dataset = MultiSizeDataset(data_dir, min_samples_per_size=500)
        
        available_sizes = multi_dataset.get_all_sizes()
        if not available_sizes:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿæ ·æœ¬çš„å°ºå¯¸")
            return False
        
        print(f"\\nâœ… å¤šå°ºå¯¸åˆ†ç±»æˆåŠŸ!")
        print(f"   å¯ç”¨å°ºå¯¸: {len(available_sizes)} ç§")
        
        # æµ‹è¯•æ¯ç§å°ºå¯¸çš„æ•°æ®é›†
        for size in available_sizes[:3]:  # åªæµ‹è¯•å‰3ç§
            dataset = multi_dataset.get_dataset(size)
            dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
            batch_states, batch_actions = next(iter(dataloader))
            
            h, w = size
            print(f"   {h:2d}Ã—{w:2d}: {len(dataset):6,} æ ·æœ¬, æ‰¹æ¬¡: {batch_states.shape}")
        
        # æµ‹è¯•æœ€å¤§æ•°æ®é›†
        largest_dataset, largest_size = multi_dataset.get_largest_dataset()
        print(f"\\nğŸ† æœ€å¤§æ•°æ®é›†: {largest_size[0]}Ã—{largest_size[1]} "
              f"({len(largest_dataset):,} æ ·æœ¬)")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤šå°ºå¯¸åˆ†ç±»æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ™ºèƒ½æ•°æ®é›†è§£å†³æ–¹æ¡ˆ')
    parser.add_argument('--data_dir', type=str, required=True)
    parser.add_argument('--method', type=str, 
                       choices=['mountain', 'multi_size', 'both'], 
                       default='both')
    
    args = parser.parse_args()
    
    if args.method in ['mountain', 'both']:
        success1 = test_mountain_padding(args.data_dir)
    else:
        success1 = True
    
    if args.method in ['multi_size', 'both']:
        success2 = test_multi_size(args.data_dir)
    else:
        success2 = True
    
    print("\\nğŸ¯ æ¨èæ–¹æ¡ˆ:")
    print("=" * 50)
    if success1:
        print("âœ… å±±è„‰å¡«å……æ–¹æ¡ˆ: ä¿ç•™æ‰€æœ‰æ•°æ®ï¼Œç¬¦åˆæ¸¸æˆé€»è¾‘")
        print("   ä½¿ç”¨æ–¹æ³•: dataset = MountainPaddedDataset(data_dir)")
    
    if success2:
        print("âœ… å¤šå°ºå¯¸åˆ†ç±»æ–¹æ¡ˆ: åˆ†åˆ«è®­ç»ƒä¸åŒå°ºå¯¸æ¨¡å‹") 
        print("   ä½¿ç”¨æ–¹æ³•: multi = MultiSizeDataset(data_dir)")
    
    print("\\nğŸ’¡ å»ºè®®:")
    print("   åˆæœŸå¼€å‘: ä½¿ç”¨å±±è„‰å¡«å……æ–¹æ¡ˆ (ç®€å•ç»Ÿä¸€)")
    print("   é«˜çº§åº”ç”¨: ç»“åˆä¸¤ç§æ–¹æ¡ˆï¼Œé’ˆå¯¹æ€§ä¼˜åŒ–")